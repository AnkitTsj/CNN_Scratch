{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ## 1. Setup and Imports\n",
    "# This section installs and imports the required libraries and modules."
   ],
   "id": "95d4880b0ed96509"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-25T09:47:07.152057Z",
     "start_time": "2025-01-25T09:47:05.041142Z"
    }
   },
   "source": [
    "from cnn import *\n",
    "from loss import *\n",
    "from validation import *"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6173ddf9ac173e13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T09:47:08.640583Z",
     "start_time": "2025-01-25T09:47:07.156206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n"
   ],
   "id": "1a3f85d4a4cd8251",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ## 2. Data Preparation\n",
    "# This section handles data loading and transformations."
   ],
   "id": "2113e3641ba5262"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T09:46:44.526010Z",
     "start_time": "2025-01-25T09:46:43.825779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "data_dir = \"./data\"\n",
    "train_dataset = datasets.CIFAR10(root=data_dir, train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ],
   "id": "c2ab57fb4aa6c053",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T09:47:16.027742Z",
     "start_time": "2025-01-25T09:47:14.407424Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "464bef424396d459",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ## 3. Model Initialization\n",
    "# Initialize the model, loss function, and optimizer."
   ],
   "id": "d2b4b9fd0517851"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T09:47:16.046715Z",
     "start_time": "2025-01-25T09:47:16.036955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,batch_size,device):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = Conv2d(3, 16, kernel_size=3, stride=1, padding=1,device = device)\n",
    "        # conv_params = {'kernel_size': 3, 'stride': 1, 'padding': 1, 'out_channels': 16}\n",
    "        conv_shape = calculate_output_shape((3,32,32),\"conv\",layer_params={'kernel_size':3,'stride':1,'padding':1,'out_channels':16})\n",
    "        self.relu = ReLU()\n",
    "        self.pool = Pool2d(kernel_size=2, stride=2,pool = \"max\")\n",
    "        pool_shape = calculate_output_shape(conv_shape,\"maxpool\",layer_params={'kernel_size':2,'stride':2,'padding':0})\n",
    "        self.fc1 = FC(batch_size,pool_shape[0]* pool_shape[1] * pool_shape[2], 10,device = device)\n",
    "        self.softmax = Softmax()\n",
    "        self.layers = nn.ModuleList([self.conv1, self.relu, self.pool, self.fc1,self.softmax][::-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - x.mean()) / (x.std() + 1e-5)  # explicit normalization\n",
    "        x,_ = self.conv1.custom_forward(x)\n",
    "        x = self.relu.custom_forward(x)\n",
    "        x = self.pool.custom_forward(x)\n",
    "        x = self.fc1.custom_forward(x)\n",
    "        x  = self.softmax.custom_forward(x,dim = -1)\n",
    "        return x,self.layers"
   ],
   "id": "7e69ddf6b610dd91",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T09:47:17.019816Z",
     "start_time": "2025-01-25T09:47:17.002075Z"
    }
   },
   "cell_type": "code",
   "source": "device",
   "id": "10e7bb7ff73733b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T09:47:17.654634Z",
     "start_time": "2025-01-25T09:47:17.510309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model = CNN(batch_size=8,device = device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_module = Loss(criterion)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "c1bd52de36ebc616",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Validate Custom Backpropagation Implementation",
   "id": "e0f6e4eb81f75b4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T09:47:18.936741Z",
     "start_time": "2025-01-25T09:47:18.692100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_input = torch.randn(4, 3, 32, 32).half().to(device)  # Replace dimensions as per your network\n",
    "sample_target = torch.randint(0, 10, (4,)).to(device)  # Replace target dimensions as needed\n",
    "validate_backward(model, sample_input, sample_target,loss_module,device= device)\n"
   ],
   "id": "620ee4a99179f1e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward pass validation: PASSED!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Validate Custom Forward Implementation",
   "id": "a8f7c40125f14d77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T09:46:47.240725Z",
     "start_time": "2025-01-25T09:46:47.232835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_input = torch.randn(4, 3, 32, 32).half().to(device)  # Replace dimensions as per your network\n",
    "validate_forward(model, sample_input,device =device)\n"
   ],
   "id": "e7d487680c2b49f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass validation: PASSED!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the model with pytorch optimizer ",
   "id": "f0a4c0eececb4d6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "losses = train_with_optimizer(model, train_loader, nn.CrossEntropyLoss(), num_epochs=5,device=device)\n",
   "id": "6a3193bfa8337e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
